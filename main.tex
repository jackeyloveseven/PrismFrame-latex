\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

\title{SynergyFrame: Orchestrating Control Signals for Photorealistic Stylization}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Yi Li\
    , Chengyan Li
}
\affiliations{
    Dalian University of Technology\
    1486591674@qq.com
}



\begin{document}
\maketitle
\begin{abstract}
Achieving photorealistic stylization in multi-guided generative tasks is often hindered by underlying conflicts between control signals, such as geometric structure and artistic style. To address this, we introduce SynergyFrame, a unified framework that transforms this conflict into collaboration. SynergyFrame is built on two pillars: (1) Physical Prior Injection, which uses novel multi-scale geometry and lighting-aware modules to establish a foundation of realism, and (2) Adaptive Semantic Fusion. At the core of this second pillar is our Semantic Sparse Attention (SSA) mechanism, a training-free attention control method. SSA intelligently adapts to different generative workflows: it overcomes signal suppression in the constrained Inpainting pipeline to enable fine-grained stylization, and it prevents content leakage and artifacts in the flexible Image-to-Image pipeline. As a plug-and-play solution, SynergyFrame significantly enhances the performance of both major workflows, establishing a new paradigm for synergistic, multi-guided image synthesis.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

The advent of diffusion models has unlocked unprecedented capabilities in controllable image generation, enabling users to guide synthesis with various inputs like text, images, and geometric maps. A paramount goal in this domain is to achieve \textbf{synergistic control}, where multiple guidance signals—such as geometric structure, lighting environment, and artistic style—are orchestrated to produce a single, coherent, and photorealistic output. However, achieving this synergy remains a significant challenge. When multiple powerful control agents, like ControlNet~\cite{zhang2023controlnet} for structure and IP-Adapter~\cite{ye2023ipadapter} for style, are deployed simultaneously, they often compete rather than collaborate. This underlying \textbf{control signal conflict} manifests as a common bottleneck, degrading the quality of results across different generative workflows.

This conflict presents distinct challenges for the two dominant paradigms in localized image editing: \textbf{Inpainting-based} and \textbf{Image-to-Image-based} stylization. The Inpainting workflow, while excellent at preserving background content, imposes strong structural and boundary constraints that tend to suppress the nuanced expression of injected style features, often leading to bland or incomplete stylization. Conversely, the Image-to-Image workflow offers greater creative freedom but is prone to generating undesirable artifacts, such as content leakage from the style reference or checkerboard patterns in unmasked regions, thereby compromising the integrity of the original content.

To resolve this fundamental challenge, we propose \textbf{SynergyFrame}, a unified and adaptive framework engineered to transform control signal conflicts into collaboration. Instead of favoring one workflow over another, SynergyFrame empowers both by addressing their respective weaknesses. Our contributions are centered around two main pillars:

\begin{enumerate}
    \item \textbf{Physical Prior Injection for Photorealism.} To ground the generation process in physical reality, we introduce two novel modules. A \textbf{Multi-Scale Geometric Awareness} module leverages multi-scale feature fusion to provide the model with a rich, detailed understanding of the target object's 3D shape. Concurrently, a \textbf{Lighting-Aware Initialization} module simulates the scene's illumination, ensuring the generated texture cohesively integrates with existing lighting and shadows. These priors form the foundation for photorealistic results in any workflow.

    \item \textbf{Adaptive Semantic Fusion for Universal Empowerment.} At the core of our framework lies \textbf{Semantic Sparse Attention (SSA)}, a novel, training-free attention mechanism. SSA intelligently adapts to different workflows to resolve their specific challenges. For \textit{Inpainting}, SSA's ability to enforce precise semantic matching at the feature level allows it to overcome the inherent signal suppression, enabling detailed and accurate style application for the first time. For \textit{Image-to-Image}, SSA, when combined with a spatial mask, effectively constrains style application, preventing content leakage and artifacts while preserving creative flexibility.
\end{enumerate}

We demonstrate that SynergyFrame, as a plug-and-play solution, significantly enhances the performance of both Inpainting and Image-to-Image pipelines for local stylization. Our work's primary contribution is not merely a new tool, but a novel perspective and a robust framework for understanding and resolving control signal conflicts in multi-guided generative models. By turning conflict into synergy, SynergyFrame elevates the state-of-the-art in producing high-fidelity, controllable, and physically-grounded visual content.

\section{Related Work}
\label{sec:related_work}

Our work is positioned at the intersection of three dynamic research areas: controllable image generation, particularly with multiple guidance signals; the pursuit of 3D-aware, photorealistic synthesis; and the development of synergistic control mechanisms that aim to decouple and harmonize generative factors.

\subsection{Controllable Generation with Multiple Guides}

The ability to guide diffusion models with precise, user-defined inputs has been a major focus of recent research. A seminal work, ControlNet~\cite{zhang2023controlnet}, introduced an architectural innovation allowing pre-trained models to be conditioned on spatial inputs like depth maps, canny edges, or human poses, enabling strong structural control. Concurrently, IP-Adapter~\cite{ye2023ipadapter} and its variants~\cite{ipadapter_plus} provided a powerful mechanism for injecting style and content from a reference image via a decoupled cross-attention strategy. 

While these tools are highly effective in isolation, their combination for complex tasks like local stylization reveals a fundamental challenge. The straightforward application of both ControlNet (for geometry) and IP-Adapter (for style) often leads to suboptimal results, as the powerful structural guidance can suppress or overwhelm the nuanced style signals. Our work directly confronts this \textbf{control signal conflict}, a problem largely overlooked by prior works that focus on the capabilities of individual control agents rather than their interplay.

\subsection{3D and Geometry-Aware Synthesis}

To enhance the realism of image generation, a growing body of work seeks to incorporate 3D and geometric priors. Methods like ZeST~\cite{cheng2024zest} leverage depth maps and simulated lighting to achieve impressive zero-shot material transfer within an inpainting framework. These approaches validate the importance of physical priors, a principle central to our work. Our \textbf{Physical Prior Injection} pillar, with its Multi-Scale Geometric Awareness and Lighting-Aware Initialization modules, builds upon this trend but with a more sophisticated approach to detail and realism.

However, works like ZeST are fundamentally bound to the inpainting paradigm. While effective for material replacement, they inherit its limitations, particularly the struggle to apply fine-grained style details under strong geometric constraints. They demonstrate the need for physical priors but do not resolve the underlying conflict between structural and stylistic controls. \textbf{SynergyFrame}, in contrast, not only incorporates advanced physical priors but also introduces the necessary mechanisms to ensure these priors can coexist and collaborate with rich stylistic guidance.

\subsection{Decoupling and Synergy in Generative Models}

Recognizing the issue of entangled representations, several recent methods have focused on decoupling different factors of generation. InstantStyle~\cite{wang2024instantstyle} and DEADiff~\cite{deadiff} propose techniques to separate style from content semantics at the feature level, enabling better preservation of the text prompt's intent during stylization. Similarly, MCA-Ctrl~\cite{yang2025multi} introduces a collaborative attention mechanism to harmonize multiple inputs, such as a reference face and a target pose, preventing identity or attribute leakage.

These methods are spiritually aligned with our goal of achieving synergy. However, they primarily address the decoupling of \textit{input modalities} (e.g., text vs. image, identity vs. structure). Our work tackles a different, more foundational level of conflict: the one arising between \textbf{different control tools and generative workflows}. Our \textbf{Adaptive Semantic Fusion} pillar, powered by Semantic Sparse Attention (SSA), provides a novel solution. It does not just disentangle style from content; it adaptively modulates the application of style itself, responding to the specific constraints of the chosen workflow (Inpainting or Image-to-Image). This allows SynergyFrame to be the first framework to systematically analyze and resolve the conflicts inherent in multi-guided, workflow-dependent generation, achieving a true, robust synergy.

\section{SynergyFrame: Methodology}
\label{sec:methodology}

To address the challenge of control signal conflict in multi-guided generation, we introduce SynergyFrame, a unified framework designed to orchestrate geometric, lighting, and style controls for photorealistic local stylization. As illustrated in Figure~\ref{fig:pipeline}, SynergyFrame is built upon two foundational pillars: (1) the injection of sophisticated physical priors to establish a foundation of realism, and (2) an adaptive attention mechanism that intelligently fuses semantic information, resolving workflow-specific challenges. Our framework is designed as a training-free, plug-and-play solution that enhances existing diffusion models.

\subsection{Pillar I: Physical Prior Injection}
\label{sec:priors}

The first pillar of SynergyFrame ensures that all subsequent generation is grounded in a high-fidelity representation of the scene's physical properties. This is achieved through two key modules.

\subsubsection{Multi-Scale Geometric Awareness}
To ensure that generated textures realistically conform to the object's 3D shape, we move beyond standard depth estimation. We employ a \textbf{Multi-Scale Depth Enhancement (MSDE)} module, which takes an initial depth map from a monocular estimator like DepthAnythingV2~\cite{yin2024depthanythingv2} and enriches it with fine-grained geometric details. As implemented in our 'GeometryEstimating.py', this module performs a weighted fusion of several features: (1) the smoothed initial depth map, which preserves overall structure; (2) high-frequency boundary features extracted from the RGB content image using a Canny edge detector, which capture sharp contours; and (3) gradient and roughness features computed from the depth map using Sobel and Laplacian filters, which describe local surface orientation. The resulting enhanced depth map provides a comprehensive geometric prior that is fed to ControlNet~\cite{zhang2023controlnet}, ensuring superior structural fidelity.
\subsubsection{Lighting-Aware Initialization}
To ensure stylistic elements integrate seamlessly with the scene's illumination, we employ a \textbf{Directional Shading Module (DSM)}. Instead of starting the diffusion process from pure noise, which would disregard the existing lighting environment, the DSM generates a physically plausible 'init-image'. It first estimates a surface normal map from our enhanced depth map. Then, using a controllable light direction, it applies a classic ambient-plus-diffuse lighting model to render a shaded version of the target object. By providing this coherently shaded image as the starting point for the diffusion process, we guide the model to generate textures and materials that are consistent with the scene's lighting, producing realistic highlights and shadows.

\subsection{Pillar II: Adaptive Semantic Fusion}
\label{sec:fusion}

The second and core innovative pillar of SynergyFrame is our mechanism for resolving control conflicts and enabling detailed style application across different workflows. This is achieved by our novel \textbf{Semantic Sparse Attention (SSA)} mechanism, implemented as a custom attention processor ('SemanticMaskedStyleAttnProcessor') that replaces the standard cross-attention processors in the U-Net.

\subsubsection{Mechanism of Semantic Sparse Attention (SSA)}
Standard cross-attention in style-injection models like IP-Adapter allows every content query to attend to all style keys. This often leads to an indiscriminate blending of style features, causing content leakage and a dilution of salient stylistic elements. SSA addresses this by enforcing sparsity in the attention mechanism. For each content query, we first compute the full matrix of similarity scores with the style keys. Then, we dynamically select only the \textbf{top-$k$} most semantically similar style keys for that specific query. Attention scores for all other keys are set to negative infinity. This ensures that each part of the content image only draws style information from the most relevant parts of the style reference, enabling a more discerning and semantically meaningful fusion. This entire process is training-free, controlled simply by the hyperparameter $k$.

\subsubsection{Workflow-Adaptive Application of SSA}
Crucially, SSA is not a one-size-fits-all solution but an \textbf{adaptive} one. It is applied differently to resolve the unique challenges of the Inpainting and Image-to-Image workflows.

\paragraph{Empowering Inpainting.} In the Inpainting workflow, the primary challenge is the suppression of style signals by the strong geometric (ControlNet) and boundary (inpainting mask) constraints. Here, SSA's primary role is to \textbf{break through this suppression}. By forcing the model to attend to the most semantically relevant style features (e.g., forcing a 'wood' patch to attend to 'wood grain' features), SSA ensures that the style information is potent and targeted enough to be expressed even under strong structural guidance. This allows for the injection of fine-grained, accurate styles in a workflow that would otherwise only produce a muted effect.

\paragraph{Enhancing Image-to-Image.} In the Image-to-Image workflow, the challenge is not suppression but a lack of constraint, which can lead to style leakage and artifacts. Here, SSA is combined with a user-provided \textbf{spatial mask}. The SSA mechanism first ensures semantic correctness, and then the spatial mask, downsampled to the feature map's resolution, is used to modulate the final attention probabilities. This application of SSA \textbf{precisely constrains} the powerful style injection to the desired region, preventing style bleed into the background and eliminating checkerboard artifacts, while still benefiting from the workflow's creative flexibility.

By intelligently adapting its function, our Semantic Sparse Attention mechanism transforms from a tool of amplification in one context to a tool of precision in another, forming the core of SynergyFrame's ability to orchestrate, rather than simply combine, control signals.

\section{Experiments}
\label{sec:experiments}

We conduct a comprehensive set of experiments to validate the core hypothesis of our work: that SynergyFrame, by orchestrating control signals, can universally enhance both Inpainting and Image-to-Image workflows for photorealistic local stylization. Our evaluation is structured to demonstrate how our framework addresses the distinct challenges of each workflow.

\subsection{Implementation Details}
Our framework is built upon the Stable Diffusion XL (SDXL) model~\cite{podell2023sdxl}, using ControlNet~\cite{zhang2023controlnet} for geometric guidance and a custom IP-Adapter~\cite{ye2023ipadapter} for style injection. For geometric prior extraction, we use the DepthAnythingV2 model~\cite{yin2024depthanythingv2}. All experiments are performed on a single NVIDIA RTX 4090 GPU with 50 denoising steps and a guidance scale of 7.5. The sparsity level $k$ for our Semantic Sparse Attention (SSA) is set to 10 unless otherwise specified.

\subsection{Empowering the Inpainting Workflow}
\label{sec:exp_inpainting}

The primary challenge in the Inpainting workflow is the suppression of style signals under strong geometric and boundary constraints. We demonstrate how SynergyFrame overcomes this limitation to achieve unprecedented stylization detail.

\subsubsection{Qualitative Comparison}
As shown in Figure~\ref{fig:inpainting_comparison}, we compare the results of a standard Inpainting pipeline (ControlNet + IP-Adapter) with the same pipeline empowered by SynergyFrame. The baseline method struggles to apply the style; the resulting texture is faint, blurry, and lacks the detailed characteristics of the style reference. In stark contrast, the output from SynergyFrame is vibrant and detailed. Our framework, particularly the SSA mechanism, successfully breaks through the signal suppression, allowing the fine-grained texture and color of the style reference to be accurately rendered onto the target object while perfectly respecting the masked boundary.

\begin{figure}[h]
  \centering
  \fbox{Figure Inpainting Comparison Placeholder}
  \caption{Qualitative comparison for the Inpainting workflow. Left to right: Content Image + Mask, Style Reference, Baseline (Standard Inpainting), Ours (SynergyFrame). Our method achieves a significantly more detailed and faithful stylization, overcoming the style suppression issue inherent in the baseline.}
  \label{fig:inpainting_comparison}
\end{figure}

\subsubsection{Ablation Study for Inpainting}
To isolate the contributions of our components within this workflow, we conduct an ablation study (Figure~\ref{fig:inpainting_ablation}). Starting with the full SynergyFrame, we individually remove its key modules:
\begin{itemize}
    \item \textbf{w/o Physical Priors:} Without our enhanced geometry and lighting priors, the generated texture, while present, fails to conform realistically to the object's surface, appearing flat and lacking correct shading.
    \item \textbf{w/o SSA:} When our Semantic Sparse Attention is replaced with standard attention, the result regresses to the baseline. The style signal is once again suppressed, demonstrating that SSA is the critical component for enabling detailed style expression in this constrained environment.
\end{itemize}

\begin{figure}[h]
  \centering
  \fbox{Figure Inpainting Ablation Placeholder}
  \caption{Ablation study for the Inpainting workflow. Left to right: Full SynergyFrame, w/o Physical Priors, w/o SSA. The results confirm that both pillars of our framework are essential for achieving high-fidelity results.}
  \label{fig:inpainting_ablation}
\end{figure}

\subsection{Enhancing the Image-to-Image Workflow}
\label{sec:exp_img2img}

The Image-to-Image workflow offers more flexibility but is prone to style leakage and artifacts. We demonstrate how SynergyFrame tames this workflow to achieve precise and clean results.

\subsubsection{Qualitative Comparison}
Figure~\ref{fig:img2img_comparison} compares a standard Image-to-Image pipeline with our SynergyFrame-enhanced version. The baseline method suffers from severe content leakage; structural elements from the style reference (e.g., the Eiffel Tower) are erroneously blended into the content image. Furthermore, without a proper mask, the style bleeds into the background. SynergyFrame, which combines SSA with a spatial mask in this workflow, completely resolves these issues. The style is applied \textit{only} to the designated region, and our SSA mechanism ensures that only the artistic texture—not the semantic content—is transferred from the reference image.

\begin{figure}[h]
  \centering
  \fbox{Figure Img2Img Comparison Placeholder}
  \caption{Qualitative comparison for the Image-to-Image workflow. Left to right: Content Image, Style Reference, Baseline (Standard Img2Img), Ours (SynergyFrame). Our method successfully prevents both content leakage and style bleeding, achieving a clean and precise stylization.}
  \label{fig:img2img_comparison}
\end{figure}

\subsubsection{Ablation Study for Image-to-Image}
Our ablation study for this workflow (Figure~\ref{fig:img2img_ablation}) confirms the dual role of our adaptive fusion mechanism:
\begin{itemize}
    \item \textbf{w/o Physical Priors:} Similar to the Inpainting case, removing the geometric and lighting priors results in a flat, unrealistic texture, proving their universal importance.
    \item \textbf{w/o SSA (Masked):} If we use a standard attention mechanism, even with a spatial mask, content leakage from the style reference persists. This demonstrates that the spatial mask alone is insufficient; SSA is crucial for achieving true semantic disentanglement and preventing the transfer of unwanted objects.
\end{itemize}

\begin{figure}[h]
  \centering
  \fbox{Figure Img2Img Ablation Placeholder}
  \caption{Ablation study for the Image-to-Image workflow. Left to right: Full SynergyFrame, w/o Physical Priors, w/o SSA (Masked). The results show that SSA is essential for preventing content leakage, a task a simple mask cannot perform alone.}
  \label{fig:img2img_ablation}
\end{figure}

\subsection{Quantitative Analysis and User Study}
To provide a more objective evaluation, we conduct a user study. We present 50 participants with pairs of images generated by our method and the corresponding baseline for both workflows. For each pair, we ask them to choose which image demonstrates (1) better overall quality, (2) more accurate style application, and (3) better preservation of the original content. The results, summarized in Table~\ref{tab:user_study}, show a strong and consistent preference for SynergyFrame across all criteria, validating its superior performance.

\begin{table}[h]
  \centering
  \fbox{Table User Study Placeholder}
  \caption{User study results showing preference percentage for SynergyFrame over baselines.}
  \label{tab:user_study}
\end{table}

\section{Conclusion and Future Work}
\label{sec:conclusion}

In this paper, we addressed the fundamental challenge of \textbf{control signal conflict} in multi-guided image generation. We introduced \textbf{SynergyFrame}, a unified framework that, for the first time, systematically orchestrates geometric, lighting, and style controls to achieve photorealistic local stylization. Our work moves beyond treating different generative workflows as monolithic options, and instead provides a solution that enhances them in a targeted, adaptive manner.

Our approach is built on two core pillars. First, our \textbf{Physical Prior Injection} modules provide a rich, physically-grounded foundation for generation, ensuring that results are geometrically accurate and cohesively lit. Second, our \textbf{Adaptive Semantic Fusion} mechanism, powered by the novel Semantic Sparse Attention (SSA), intelligently resolves the unique challenges of different workflows. It breaks through signal suppression in the constrained \textit{Inpainting} pipeline and prevents content leakage in the flexible \textit{Image-to-Image} pipeline.

By transforming control signal conflicts into a collaborative synergy, SynergyFrame consistently and significantly elevates the performance of both major local stylization workflows. Our primary contribution is thus not only a tool for superior image synthesis, but also a new perspective and a robust methodology for resolving conflicts in the increasingly complex landscape of multi-guided generative models.

\paragraph{Future Work.} The synergistic principles of our framework have promising potential for broader applications. Future work could explore: (1) extending SynergyFrame to \textbf{video stylization}, to harmonize control signals while maintaining temporal consistency; (2) integrating our framework into \textbf{3D rendering pipelines} as an advanced material generation tool for digital assets; and (3) investigating the orchestration of an even wider array of control signals, such as text or pose, within our synergistic framework.

\bibliography{references}

\end{document}
